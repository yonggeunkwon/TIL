{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-batch gradient descent\n",
    "\n",
    "- Gradient descent는 1개의 데이터를 기준으로 미분함\n",
    "- 그러나 일반적인 Gradient descent는 (full) batch Gradient descent 인 경우가 많음\n",
    "- 모든 데이터 셋을 가지고 한번에 미분값을 구하는 게 Full-batch Gradient descent임\n",
    "\n",
    "${{\\partial}J\\over{\\partial}{w_n}} = \\frac{1}{m}\\sum{({{w_1}x^{(i)}}+w_0-y^{(i)}){x_n}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 장점\n",
    "- Single Gradient descent에 비해서 빠른 속도\n",
    "- 안정적인 Cost 함수 수렴\n",
    "\n",
    "#### 단점\n",
    "- 메모리 문제 발생 (아주 큰 데이터를 처리할 수 없음 - Full batch 사용 불가)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent (확률적 경사하강법)\n",
    "\n",
    "- 랜덤하게 경사하강을 함\n",
    "- Single gradient descent는 local optimum에 빠질 가능성이 높음\n",
    "- Stochastic gradient descent 에서는 여러곳에서 gradient descent를 시작해 local optimum에 빠질 가능성을 낮춤"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빈번한 업데이트가 발생하여 모델의 성능 및 개선 속도를 확인할 수 있다\n",
    "- 일부 문제에 대해 더 빨리 수렴한다\n",
    "- local optimum에 빠지는 것을 방지한다\n",
    "- 대용량 데이터를 다루는 경우에는 시간이 오래 걸린다\n",
    "- 더 이상 cost가 줄어들지 않는 시점의 발견이 어렵다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch SGD\n",
    "\n",
    "- 한번에 일정량의 데이터를 랜덤하게 뽑아서 학습함\n",
    "- SGD와 Batch GD를 혼합한 기법임\n",
    "- 가장 일반적으로 많이 쓰이는 기법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch & Batch-size\n",
    "\n",
    "- Epoch은 전체 데이터가 한번 Training 될 때 카운팅 되는 횟수\n",
    "- Full-batch를 n번 실행하면 n epoch이 됨\n",
    "- Batch-size : 한번에 학습되는 데이터의 개수\n",
    "- ex) 총 5,120개의 Training data에 512 batch-size면 몇 번 학습을 해야 1 epoch이 되는가? -> 10번"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch SGD 의 절차\n",
    "\n",
    "1. X를 셔플해줌\n",
    "2. BS에 Batch-size를 저장해줌\n",
    "3. 전체데이터의 개수 / BS 를 해서 NB (number of batch - 학습 횟수)를 저장해줌\n",
    "4. batch마다 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
